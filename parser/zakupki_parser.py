import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urlparse, parse_qs

BASE_URL = "https://zakupki.gov.ru"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# üìå –ü–æ–∏—Å–∫ —Ç–µ–Ω–¥–µ—Ä–æ–≤ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
def search_tenders(keyword: str, page: int = 1):
    url = f"{BASE_URL}/epz/order/extendedsearch/results.html?pageNumber={page}&searchString={keyword}"
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞: {e}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    results = []

    tender_blocks = soup.find_all("div", class_="search-registry-entry-block")

    for block in tender_blocks:
        number_tag = block.find("div", class_="registry-entry__header-mid__number")
        number = number_tag.text.strip() if number_tag else "–ë–µ–∑ –Ω–æ–º–µ—Ä–∞"

        name_tag = block.find("div", class_="registry-entry__body-value")
        name = name_tag.text.strip() if name_tag else "–ë–µ–∑ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è"

        price_tag = block.find("div", class_="price-block__value")
        price = price_tag.text.strip() if price_tag else "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

        date_tag = block.find("div", class_="data-block__value")
        end_date = date_tag.text.strip() if date_tag else "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

        link_tag = block.find("a", href=True)
        link = f"{BASE_URL}{link_tag['href']}" if link_tag else url

        results.append({
            "number": number,
            "name": name,
            "price": price,
            "end_date": end_date,
            "link": link
        })

    return results

# üìå –î–æ—Å—Ç–∞—ë—Ç –Ω–æ–º–µ—Ä –∑–∞–∫—É–ø–∫–∏ –∏–∑ URL
def extract_reg_number(link):
    parsed = urlparse(link)
    params = parse_qs(parsed.query)
    return params.get("regNumber", [None])[0]

# üìå –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–¥–ø–∏—Å–∏ (–≠–¶–ü)
def get_signature_data(reg_number):
    sig_url = f"{BASE_URL}/epz/order/notice/printForm/listModal.html?regNumber={reg_number}"

    try:
        resp = requests.get(sig_url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–¥–ø–∏—Å–∏ –¥–ª—è {reg_number}: {e}")
        return {}

    soup = BeautifulSoup(resp.text, "html.parser")
    text = soup.get_text(separator="\n")
    data = {"–§–ò–û": "", "–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è": "", "–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç": "", "–ü–µ—Ä–∏–æ–¥ –¥–µ–π—Å—Ç–≤–∏—è": "", "–ü–æ–¥–ø–∏—Å—å": ""}

    lines = [line.strip() for line in text.split('\n') if line.strip()]
    for line in lines:
        if line.endswith("."):
            data["–§–ò–û"] = line
        if "–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è:" in line:
            data["–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è"] = line.split("–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è:")[-1].strip()
        if "–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç:" in line:
            data["–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç"] = line.split("–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç:")[-1].strip()
        if "–ü–µ—Ä–∏–æ–¥ –¥–µ–π—Å—Ç–≤–∏—è —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞:" in line:
            data["–ü–µ—Ä–∏–æ–¥ –¥–µ–π—Å—Ç–≤–∏—è"] = line.split("–ü–µ—Ä–∏–æ–¥ –¥–µ–π—Å—Ç–≤–∏—è —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞:")[-1].strip()

    textarea = soup.find('textarea')
    if textarea:
        data["–ü–æ–¥–ø–∏—Å—å"] = textarea.text.strip()

    return data

# üìå –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–¥–µ—Ä–æ–≤ –≤ CSV-—Ñ–∞–π–ª
def save_to_csv(tenders, filename="tenders_with_signatures.csv"):
    with open(filename, mode="w", encoding="utf-8-sig", newline="") as file:
        writer = csv.writer(file)
        writer.writerow([
            "–ù–æ–º–µ—Ä –∑–∞–∫—É–ø–∫–∏", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–¶–µ–Ω–∞", "–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è", "–°—Å—ã–ª–∫–∞",
            "–§–ò–û –ø–æ–¥–ø–∏—Å–∞–Ω—Ç–∞", "–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è", "–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç", "–ü–µ—Ä–∏–æ–¥ –¥–µ–π—Å—Ç–≤–∏—è", "–¢–µ–∫—Å—Ç –ø–æ–¥–ø–∏—Å–∏"
        ])

        for tender in tenders:
            writer.writerow([
                tender.get("number", ""),
                tender.get("name", ""),
                tender.get("price", ""),
                tender.get("end_date", ""),
                tender.get("link", ""),
                tender.get("–§–ò–û", ""),
                tender.get("–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è", ""),
                tender.get("–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç", ""),
                tender.get("–ü–µ—Ä–∏–æ–¥ –¥–µ–π—Å—Ç–≤–∏—è", ""),
                tender.get("–ü–æ–¥–ø–∏—Å—å", "")
            ])

# üìå –ì–ª–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞: –ø–æ–∏—Å–∫, –ø–æ–¥–ø–∏—Å–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
def parse_tenders_and_save(keyword, page=1, output_file="tenders_with_signatures.csv"):
    tenders = search_tenders(keyword, page)
    for tender in tenders:
        reg_number = extract_reg_number(tender["link"])
        if reg_number:
            sig_data = get_signature_data(reg_number)
            tender.update(sig_data)
    save_to_csv(tenders, output_file)
