import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urlparse, parse_qs

BASE_URL = "https://zakupki.gov.ru"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ğŸ“Œ ĞŸĞ¾Ğ¸ÑĞº Ñ‚ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¼Ñƒ ÑĞ»Ğ¾Ğ²Ñƒ
def search_tenders(keyword: str, page: int = 1):
    url = f"{BASE_URL}/epz/order/extendedsearch/results.html?pageNumber={page}&searchString={keyword}"
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
    except Exception as e:
        print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°: {e}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    results = []

    tender_blocks = soup.find_all("div", class_="search-registry-entry-block")

    for block in tender_blocks:
        number_tag = block.find("div", class_="registry-entry__header-mid__number")
        number = number_tag.text.strip() if number_tag else "Ğ‘ĞµĞ· Ğ½Ğ¾Ğ¼ĞµÑ€Ğ°"

        name_tag = block.find("div", class_="registry-entry__body-value")
        name = name_tag.text.strip() if name_tag else "Ğ‘ĞµĞ· Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"

        price_tag = block.find("div", class_="price-block__value")
        price = price_tag.text.strip() if price_tag else "Ğ¦ĞµĞ½Ğ° Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ°"

        date_tag = block.find("div", class_="data-block__value")
        end_date = date_tag.text.strip() if date_tag else "Ğ”Ğ°Ñ‚Ğ° Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ°"

        link_tag = block.find("a", href=True)
        link = f"{BASE_URL}{link_tag['href']}" if link_tag else url

        results.append({
            "number": number,
            "name": name,
            "price": price,
            "end_date": end_date,
            "link": link
        })

    return results

# ğŸ“Œ Ğ”Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ·Ğ°ĞºÑƒĞ¿ĞºĞ¸ Ğ¸Ğ· URL
def extract_reg_number(link):
    parsed = urlparse(link)
    params = parse_qs(parsed.query)
    return params.get("regNumber", [None])[0]

# ğŸ“Œ ĞŸĞ°Ñ€ÑĞ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ (Ğ­Ğ¦ĞŸ)
def get_signature_data(reg_number):
    sig_url = f"{BASE_URL}/epz/order/notice/printForm/listModal.html?regNumber={reg_number}"

    try:
        resp = requests.get(sig_url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ´Ğ»Ñ {reg_number}: {e}")
        return {}

    soup = BeautifulSoup(resp.text, "html.parser")
    text = soup.get_text(separator="\n")
    data = {"Ğ¤Ğ˜Ğ": "", "ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ": "", "Ğ¡ĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚": "", "ĞŸĞµÑ€Ğ¸Ğ¾Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ": "", "ĞŸĞ¾Ğ´Ğ¿Ğ¸ÑÑŒ": ""}

    lines = [line.strip() for line in text.split('\n') if line.strip()]
    for line in lines:
        if line.endswith("."):
            data["Ğ¤Ğ˜Ğ"] = line
        if "ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ:" in line:
            data["ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"] = line.split("ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ:")[-1].strip()
        if "Ğ¡ĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚:" in line:
            data["Ğ¡ĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚"] = line.split("Ğ¡ĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚:")[-1].strip()
        if "ĞŸĞµÑ€Ğ¸Ğ¾Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ°:" in line:
            data["ĞŸĞµÑ€Ğ¸Ğ¾Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ"] = line.split("ĞŸĞµÑ€Ğ¸Ğ¾Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ°:")[-1].strip()

    textarea = soup.find('textarea')
    if textarea:
        data["ĞŸĞ¾Ğ´Ğ¿Ğ¸ÑÑŒ"] = textarea.text.strip()

    return data

# ğŸ“Œ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ‚ĞµĞ½Ğ´ĞµÑ€Ğ¾Ğ² Ğ² CSV-Ñ„Ğ°Ğ¹Ğ»
def save_to_csv(tenders, filename="tenders_with_signatures.csv"):
    with open(filename, mode="w", encoding="utf-8-sig", newline="") as file:
        writer = csv.writer(file)
        writer.writerow([
            "ĞĞ¾Ğ¼ĞµÑ€ Ğ·Ğ°ĞºÑƒĞ¿ĞºĞ¸", "ĞĞ°Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ", "Ğ¦ĞµĞ½Ğ°", "Ğ”Ğ°Ñ‚Ğ° Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ğ½Ğ¸Ñ", "Ğ¡ÑÑ‹Ğ»ĞºĞ°",
            "Ğ¤Ğ˜Ğ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ°Ğ½Ñ‚Ğ°", "ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ", "Ğ¡ĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚", "ĞŸĞµÑ€Ğ¸Ğ¾Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ", "Ğ¢ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸"
        ])

        for tender in tenders:
            writer.writerow([
                tender.get("number", ""),
                tender.get("name", ""),
                tender.get("price", ""),
                tender.get("end_date", ""),
                tender.get("link", ""),
                tender.get("Ğ¤Ğ˜Ğ", ""),
                tender.get("ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ", ""),
                tender.get("Ğ¡ĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚", ""),
                tender.get("ĞŸĞµÑ€Ğ¸Ğ¾Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ", ""),
                tender.get("ĞŸĞ¾Ğ´Ğ¿Ğ¸ÑÑŒ", "")
            ])

# ğŸ“Œ Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ°: Ğ¿Ğ¾Ğ¸ÑĞº, Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
def parse_tenders_and_save(keyword, page=1, output_file="tenders_with_signatures.csv"):
    tenders = search_tenders(keyword, page)
    for tender in tenders:
        reg_number = extract_reg_number(tender["link"])
        if reg_number:
            sig_data = get_signature_data(reg_number)
            tender.update(sig_data)
    save_to_csv(tenders, output_file)

# ğŸ“ ĞĞ±Ñ‘Ñ€Ñ‚ĞºĞ° Ğ´Ğ»Ñ FastAPI
def parse_data(keyword: str, page: int = 1) -> dict:
    tenders = search_tenders(keyword, page)
    for tender in tenders:
        reg_number = extract_reg_number(tender["link"])
        if reg_number:
            sig_data = get_signature_data(reg_number)
            tender.update(sig_data)
    return {"results": tenders}
